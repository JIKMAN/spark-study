## 스파크의 철학

1. 통합
   * 대규모 데이터 애플리케이션 개발에 필요한 통합 플랫폼을 제공
   * 시간이 지나면서 API가 더욱 확장, 다양한 라이브러리를 제공
2. 컴퓨팅 엔진
   * 저장소 시스템의 데이터를 연산하는 역할만 수행할 뿐 영구 저장소 역할은 수행 X
   * 애저 스토리지, S3, 하둡, 카산드라, 카프카 등의 저장소를 지원
3. 라이브러리
   * 스파크SQL, MLlib, 스파크 스트리밍, GraphX 등 다양한 라이브러리 제공
   * 그 외에도 다양한 저장소 커넥터, 수많은 외부 오픈소스 라이브러리[spark-packages.org](spark-packages.org)

### 스파크 아키텍처

전체적인 동작과정

클라이언트는 클러스터 매니저에 스파크 애플리케이션을 제출

제출 받은 클러스터 매니저는 애플리케이션 실행에 필요한 자원을 할당

### 스파크 애플리케이션

드라이버 프로세스와, 다수의 익스큐터 프로세스로 구성

#### 드라이버 프로세스

* 클러스터 노드 중 하나에서 실행되며 main() 함수를 실행
* 애플리케이션 정보의 유지 관리, 사용자 프로그램이나 입력에 대한 응답, 전반적인 익스큐터 프로세스의ㅡ 작업과 관련된 분석, 배포, 스케줄링 역할

#### 익스큐터

* 드라이버 프로세스가 할당한 작업을 수행

### 스파크 API

저수준의 비구조적 API 와 고수준의 구조적API 를 제공한다.

### SparkSession

* 하나의 스파크세션은 하나의 애플리케이션을 처리
* 사용자가 정의한 처리 명령을 클러스터에서 실행

### DataFrame

대표적인 구조적 API로 테이블의 데이터를 로우와 컬럼으로 단순하게 표현

* 스키마 : 컬럼과 컬럼의 타입을 정의

> **파티션**
>
> 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 단위로 데이터를 분할, 파티션은 클러스터의 물리적 머신에 존재하는 로우의 집합을 의미



### 트랜스포메이션

논리적 실행 계획

스파크 데이터 구조는 불변성immutable 즉, 한번 생성되면 변경할 수 없다.

변경을 원하면 트랜스포메이션을 이용해 변경 가능

```scala
val myRange = spark.range(1000).toDF("number")

val divisBy2 = myRange.where("number % 2 = 0") // 트랜스포메이션
```

* 액션을 호출하지 않으면 실제 트랜스포메이션을 수행하지 않는다.

* 트랜스포메이션 2가지 유형 : narrow dependency / wide dependency

  > narraw dependency
  >
  > 하나의 입력 파티션이 하나의 출력 파티션에만 영향을 미친다. (1:1)
  >
  > wide dependency
  >
  > 하나의 입력 파티션이 여러 출력 파티션에 영향을 미친다. (1:N)



### 지연연산

 lazy evaluation (지연 연산)이란 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식

스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성, 마지막 순간가지 대기하다가 물리적 실행 계획으로 컴파일

(스파크는 이 필터를 데이터 소스로 위임하는 최적화 작업을 자동 수행 ex. where절의 처리를 DB로  위임)

### 

### 액션

실제 연산을 수행하는 명령, 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령

```scala
divisBy2.count()
```

액션을 지정하면 스파크 job이 시작됨.

액션의 종류

* 콘솔에서 데이터를 보는 액션
* 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
* 출력 데이터소스에 저장하는 액션



### 스파크 UI

드라이브 노드의 4040 포트 `http://localhost:4040` - 스파크 잡을 튜닝하고 디버깅 할때 유용

스파크 잡은 개별 액션에 의해 트리거 되는 다수의 트랜스포메이션으로 이루어져 있으며 스파크 UI로 잡을 모니터링 할 수 있다.



### 정리

```scala
val flightData2015 = spark
	.read
	.option("inferSchema", "true")
	.option("header", "true")
	.csv(path)
```



```scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

사용자는 물리적 데이터를 직접 다루지 않지만, 위와 같이 셔플 파티션 파라미터와 같은 물리 속성으로 물리적 실행 특성을 제어한다. 이 설정 변경을 통해 파티션 수가 변경되고, 값을 변경하면 런타임이 크게 달라질 수 있다.



### DataFrame 과 SQL

사용자가 SQL이나 DataFrame으로 비지니스 로직을 표현하면 스파크에서 실제 코드를 실행하기 전에 그 로직을 기본 실행 계획(explain 메서드로 확인 가능)으로 컴파일한다.

스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시테이블)로 등록한 후 SQL 쿼리를 사용할 수 있다. 스파크는 SQL 쿼리를 DataFram 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없다.

`CreateOrReplaceTempView` : 모든 DataFrame을 테이블이나 뷰로 만들 수 있는 메서드



### 운영용 애플리케이션 실행

```scala
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local \
  ./examples/jars/spark-examples_2.11-2.2.0.jar 10
```





정적 데이터셋의 데이터를 분석하여 DataFrame처리하는 과정

```scala
val staticDataFrame = spark.read.format("csv")
	.option("header", "true")
	.option("inferSchema", "true")
	.load(path)

staticDataFrame.createOrReplaceTempView("retail_data")
val staticSchema = staticDataFrame.schema

import org.apache.spark.sql.functions.{window, col}

staticDataFram.selectExpr(
	"CustomerId",
	"(UnitPrice * Quantityu) as total_cost",
	"InvoiceDate")
	.groupBy(
        col("CustomerId"), window(col("InvoiceDate"), "1 day"))
	.sum("total_cost")
	.show(5)
```

로컬에선 많은 익스큐터가 필요하지 않기 때문에 파티션수를 조절

```scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```







## 구조적 API

* Dataset
* DataFrame

* SQL 테이블과 뷰

배치와 스트리밍 처리에서 구조적 API를 사용할 수 있다.

핵심 기본 개념

* 타입형/비타입형 API의 개념과 차이점
* 핵심 용어
* 스파크가 구조적API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식



### DataFrame과 Dataset

이 둘은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션이다. DataFrame과 Dataset은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며 불변성을 가짐. DataFrame에 액션을 호출하면 스파크는 트랜스포메이션을 실제로 실행하고 결과를 반환함.

### 스키마

DataFrame의 컬럼명과 데이터 타입을 정의한다. 데이터 소스에서 얻거나 직접 정의할 수 있음.



### 스파크의 구조적 데이터 타입

스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 **카탈리스트** 엔진을 사용한다. 스파크는 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있다.

```scala
// 스파크 카탈리스트 엔진에서 스파크의 데이터 타입으로 변환하여 명령을 처리하기 때문에
// 스칼라나 파이썬이 아닌 스파크의 덧셈 연산을 수행함
val df = spark.range(500).toDF("number")
df.select(df.col("number") + 10)
```

#### DataFrame 과 Dataset비교

구조적 API : `비타입형`인 DataFrame / `타입형`인 Dataset

> DataFrame은 스키마에 명시된 데이터 타입의 일치 여부를 `runtime`이 되어서야 확인하는 반면, Dataset은 `compile time`에 확인한다.
>
> Row 타입으로 구성된 DataFrame을 사용하면 garbage collection과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 효율적인 연산 가능

✔️ DataFrame을 사용하면 스파크의 최적화된 내부 포맷을 사용할 수 있다.

#### 스파크의 데이터 타입

```scala
// 스칼라에서 사용 시 예제
import org.apache.spark.sql.types._
val b = ByteType
```



### 구조적 API의 실행과정

1. DataFrame/Dataset/SQL을 이용해 코드를 작성
2. 스파크가 논리적 실행 계획으로 변환
3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크 클러스터에서 물리적 실행 계획(RDD 처리)을 실행

* 논리적 실행계획 단계에서는 추상적 트랜스포메이션만 표현한다. 이 단계에서는 드라이버나 익스큐터의 정보를 고려하지 않는다. 카탈리스트 옵티마이저는 논리적 실행 계획을 최적화하는 규칙의 모음이다.
* 물리적 실행 계획 단계에서는 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의한다. 비용모델을 이용해 최적의 물리적 실행 계획을 선택한다.
* 실행 계획을 선정한 다음 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행하고 처리 결과를 사용자에게 반환한다.



## 구조적 API의 기본 연산

```scala
// DataFrame 생성
val df = spark.read.format("json")
	.load(path)

// 스키마 확인
df.printSchema()
```

### 스키마

DataFrame의 컬럼명과 데이터 타입을 정의한다. 

```scala
// DataFrame에 스키마를 만들고 적용하는 예제
    val mySchema = StructType(Array(
      StructField("DEST_COUNTRY_NAME", StringType, true),
      StructField("ORIGIN_COUNTRY_NAME", StringType, true),
      StructField("count", LongType, false,
        Metadata.fromJson("{\"hello\":\"world\"}"))
    ))
```

